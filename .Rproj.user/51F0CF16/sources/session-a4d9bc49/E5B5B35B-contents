---
title: "Lab 4: LDA - QDA - Naive Bayes"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```



```{r}
library(tidymodels)
library(ISLR) # For the Smarket data set
library(ISLR2) # For the Bikeshare data set
library(discrim)
library(poissonreg)
library(corrr)
library(corrplot)
library(klaR) # for naive bayes
tidymodels_prefer()
```

### Data

We'll keep working with the `Smarket` data set for this lab, which contains daily percentage returns for the S&P 500 stock index between 2001 and 2005. It contains 1,250 observations on 8 numeric variables, plus a factor variable called `Direction` which has two levels, "Up" and "Down".

The other data set we'll work with for this lab is the `OJ`, or Orange Juice, data set. It contains 1,070 observations, each of which represents a customer's purchase of either Citrus Hill or Minute Maid Orange Juice. `Purchase` is our outcome variable, which indicates the brand purchased. We'll attempt to predict it using the prices charged for each brand, the discounts offered, and a metric of customer loyalty.




As in the past lab we'll split each data set, stratifying on the outcomes, `Direction` and `Purchase`. 

```{r}
set.seed(3435)
smarket_split <- initial_split(Smarket, prop = 0.70,
                                strata = Direction)
smarket_train <- training(smarket_split)
smarket_test <- testing(smarket_split)

set.seed(3435)
oj_split <- initial_split(OJ, prop = 0.70,
                                strata = Purchase)
oj_train <- training(oj_split)
oj_test <- testing(oj_split)
```


### Creating a Recipe

First, we want to create a recipe to represent the model we'll be fitting -- one for `Direction` and one for `Purchase`. Neither model has categorical predictors, so we won't need to include `step_dummy`.

```{r}
smarket_recipe <- recipe(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + 
                           Lag5 + Volume, data = smarket_train)

oj_recipe <- recipe(Purchase ~ PriceCH + PriceMM + DiscCH + DiscMM +
         LoyalCH + PctDiscMM + PctDiscCH, data = oj_train)
```



```{r, echo=FALSE}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
```



```{r, echo=FALSE}
smarketlog_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(smarket_recipe)

smarketlog_fit <- fit(smarketlog_wkflow, smarket_train)

ojlog_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(oj_recipe)

ojlog_fit <- fit(ojlog_wkflow, oj_train)
```


```{r, echo=FALSE}
log_reg_acc_smarket <- augment(smarketlog_fit, new_data = smarket_train) %>%
  accuracy(truth = Direction, estimate = .pred_class)


log_reg_acc_oj <- augment(ojlog_fit, new_data = oj_train) %>%
  accuracy(truth = Purchase, estimate = .pred_class)
```

We will now go on to fit three models to the **training** data sets: A linear discriminant analysis (LDA) model, a quadratic discriminant analysis (QDA) model, and a naive Bayes model.


## LDA

The beauty of `tidymodels` is that we only need to set up the recipe once. Then fitting any number of additional model classes can be done with only a few lines of code:

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

smarketlda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(smarket_recipe)

smarketlda_fit <- fit(smarketlda_wkflow, smarket_train)

ojlda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(oj_recipe)

ojlda_fit <- fit(ojlda_wkflow, oj_train)
```

### Assessing Performance

We can view a confidence matrix and calculate accuracy on the **training data**:

```{r}
augment(smarketlda_fit, new_data = smarket_train) %>%
  conf_mat(truth = Direction, estimate = .pred_class) 

augment(ojlda_fit, new_data = oj_train) %>%
  conf_mat(truth = Purchase, estimate = .pred_class) 
```

```{r}
smarketlda_acc <- augment(smarketlda_fit, new_data = smarket_train) %>%
  accuracy(truth = Direction, estimate = .pred_class)
smarketlda_acc

ojlda_acc <- augment(ojlda_fit, new_data = oj_train) %>%
  accuracy(truth = Purchase, estimate = .pred_class)
ojlda_acc
```

#### Activities

-   Compare the results of the LDA models to the results of the logistic regression models from the last lab.

## QDA

Again, fitting any number of additional model classes can be done with only a few lines of code:

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

smarketqda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(smarket_recipe)

smarketqda_fit <- fit(smarketqda_wkflow, smarket_train)

ojqda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(oj_recipe)

ojqda_fit <- fit(ojqda_wkflow, oj_train)
```

### Assessing Performance

And again we can view confidence matrices and calculate accuracies on the **training data**:

```{r}
augment(smarketqda_fit, new_data = smarket_train) %>%
  conf_mat(truth = Direction, estimate = .pred_class) 

smarketqda_acc <- augment(smarketqda_fit, new_data = smarket_train) %>%
  accuracy(truth = Direction, estimate = .pred_class)
smarketqda_acc

augment(ojqda_fit, new_data = oj_train) %>%
  conf_mat(truth = Purchase, estimate = .pred_class) 

ojqda_acc <- augment(ojqda_fit, new_data = oj_train) %>%
  accuracy(truth = Purchase, estimate = .pred_class)
ojqda_acc
```

## Naive Bayes

Finally, we'll fit a Naive Bayes model to the **training data**. For this, we will be using the `naive_bayes()` function to create the specification and set the `usekernel` argument to `FALSE`. This means that we are assuming that the predictors are drawn from Gaussian distributions.

```{r}
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

smarketnb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(smarket_recipe)

smarketnb_fit <- fit(smarketnb_wkflow, smarket_train)

ojnb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(oj_recipe)

ojnb_fit <- fit(ojnb_wkflow, oj_train)
```

### Assessing Performance

We can view confidence matrices and calculate accuracies on the **training data**:

```{r}
augment(smarketnb_fit, new_data = smarket_train) %>%
  conf_mat(truth = Direction, estimate = .pred_class)

smarketnb_acc <- augment(smarketnb_fit, new_data = smarket_train) %>%
  accuracy(truth = Direction, estimate = .pred_class)
smarketnb_acc

augment(ojnb_fit, new_data = oj_train) %>%
  conf_mat(truth = Purchase, estimate = .pred_class)

ojnb_acc <- augment(ojnb_fit, new_data = oj_train) %>%
  accuracy(truth = Purchase, estimate = .pred_class)
ojnb_acc
```

## Comparing Model Performance

Now we can make a table of the accuracy rates from these three models in addition to the results from the logistic regression that we ran during the last lab. This comparison is based on accuracy rates on the  **training data** for each of the two data sets.

For the `Smarket` data:

```{r}
accuracies <- c(log_reg_acc_smarket$.estimate, 
                smarketlda_acc$.estimate, 
                smarketnb_acc$.estimate, 
                smarketqda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>%
  arrange(-accuracies)
```

For the `OJ` data set:

```{r}
accuracies <- c(log_reg_acc_oj$.estimate, ojlda_acc$.estimate, 
                ojnb_acc$.estimate, ojqda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>%
  arrange(-accuracies)
```

#### Activities

-   Which model performed the best on the training data for each data set?
  QDA and logistic regression
-   Which model would you choose? Why?
  I would choose QDA

## Fitting to Testing Data

Since the QDA model performed slightly better for `Smarket`, we'll go ahead and fit it to the testing data. Note that what we are doing -- that is, selecting a model based on its *training accuracy* -- is **not** ideal practice, as we've discussed in class. The model with the highest training accuracy is **not** guaranteed to have high testing accuracy.

In future weeks, we'll cover how to use cross-validation to get an estimate of our models' performance on *testing data* and use that estimate for selecting a model instead. We choose based on training accuracy here **only because** we haven't discussed cross-validation yet. Choosing a model based on training performance is ***not a good idea in general*****.**

Since we've chosen a model, however -- QDA -- we can view its confusion matrix using the **testing** data:

```{r}
augment(smarketqda_fit, new_data = smarket_test) %>%
  conf_mat(truth = Direction, estimate = .pred_class) 
```

We can also look at its **testing** accuracy. Here, we add two other metrics, sensitivity and specificity, out of curiosity:

```{r}
multi_metric <- metric_set(accuracy, sensitivity, specificity)

augment(smarketqda_fit, new_data = smarket_test) %>%
  multi_metric(truth = Direction, estimate = .pred_class)
```

Finally, let's look at an ROC curve on the testing data:

```{r}
augment(smarketqda_fit, new_data = smarket_test) %>%
  roc_curve(Direction, .pred_Down) %>%
  autoplot()

augment(smarketqda_fit, new_data = smarket_test)
```

We'll fit the logistic regression to the `OJ` testing set, since it also performed best on the training set, with the same caveats, and view the same things:

```{r}
augment(ojlog_fit, new_data = oj_test) %>%
  conf_mat(truth = Purchase, estimate = .pred_class) 
multi_metric <- metric_set(accuracy, sensitivity, specificity)
augment(ojlog_fit, new_data = oj_test) %>%
  multi_metric(truth = Purchase, estimate = .pred_class)
augment(ojlog_fit, new_data = oj_test) %>%
  roc_curve(Purchase, .pred_CH) %>%
  autoplot()
```

#### Activities

-   What do the sensitivity and specificity values mean? Interpret them in terms of the concepts and data.
  
-   How well did the models perform on the **testing** data?
  not as well as on the training
-   Why do you think the models performed like they did?
  its so hard to predict stocks, but not as hard to predict OJ
-   Which model performed better?
  the logistic regression

discrim analysis:
- goal: classify example into k diff classes
- Y is respnse variable / class
- observe covariance and predictors, assess probability that Y belongs to a certain class
  P(Y = k | X1 = x1, ..., xp) from slides
  - bayes lets you flip the two probability things the conditional
- estimate pi_k with training data
- impose multivariate normal 
univariate normal: ~ N(mu, sigma sq)
mutlivariate normal: ~ N(mu with arrow over it for vector, sigma uppercase like the cumsum symbol) -- uppercase sigma is a matrix of covariances
- k classes
- for LDA: uppercase sigma is fixed
- for QDA: uppercase sigma is not fixed, so have sigma_1, sigma_2, ...., sigma_k
    - so also estimating covariance matrix for QDA
- saying our joint dists are multivariate normal
  - or univariate normal if x=1
- estimate RHS of eqn with training data, then use bayes thm to get LHS probability

Naive bayes:
- instead of putting a distribution on f, the assumption is that f(x) is indep in diff components
- so x1, x2, ..., xp are indep RVs
- so you can write joint pdf as producct of each one like f(x1)*f(x2)*...*f(xp)
  - so dont have to worry about joint dist, only marginal
- 


## Resources

The free book [Tidy Modeling with R](https://www.tmwr.org/) is strongly recommended.

## Source

Several parts of this lab come directly from the ["ISLR Tidymodels Labs"](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/classification.html). Credit to Emil Hvitfeldt for writing and maintaining the open-source book.
