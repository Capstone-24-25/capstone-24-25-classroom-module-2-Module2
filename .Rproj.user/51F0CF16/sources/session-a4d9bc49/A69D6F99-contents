---
title: "Homework 2"
author: "PSTAT 131/231"
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Linear Regression and KNN

For this assignment, we will be working with a data set from the UCI (University of California, Irvine) Machine Learning repository ([see website here](http://archive.ics.uci.edu/ml/datasets/Abalone)). The full data set consists of $4,177$ observations of abalone in Tasmania. (Fun fact: [Tasmania](https://en.wikipedia.org/wiki/Tasmania "Tasmania") supplies about $25\%$ of the yearly world abalone harvest.)

![*Fig 1. Inside of an abalone shell.*](images/17612037-abalone-shell-inside.jpg){width="309"}

The age of an abalone is typically determined by cutting the shell open and counting the number of rings with a microscope. The purpose of this data set is to determine whether abalone age (**number of rings + 1.5**) can be accurately predicted using other, easier-to-obtain information about the abalone.

The full abalone data set is located in the `\data` subdirectory. Read it into *R* using `read_csv()`. Take a moment to read through the codebook (`abalone_codebook.txt`) and familiarize yourself with the variable definitions.

Make sure you load the `tidyverse` and `tidymodels`!

```{r}
library(tidyverse)
library(tidymodels)
abalone <- read_csv('/Users/tessivinjack/Documents/PSTAT 131/homework-2/data/abalone.csv')
abalone
```


### Question 1

Your goal is to predict abalone age, which is calculated as the number of rings plus 1.5. Notice there currently is no `age` variable in the data set. Add `age` to the data set.

Assess and describe the distribution of `age`.

```{r}
abalone <- abalone %>% mutate(age = rings + 1.5)
hist(abalone$age, main = 'Distribution of Abalone Age')
```
The distribution of the abalone age shows us that the most common age for abalones is around around 10 years. It follows a relatively normal, bell-shaped distribution that is slightly skewed right with most of the data falling between 7 and 15 years.

### Question 2

Split the abalone data into a training set and a testing set. Use stratified sampling. You should decide on appropriate percentages for splitting the data.

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*

```{r}
set.seed(114424)

abalone_split <- initial_split(abalone, prop = 0.80,
                               strata = age)
abalone_train <- training(abalone_split)
abalone_test <- testing(abalone_split)
```

### Question 3

Using the **training** data, create a recipe predicting the outcome variable, `age`, with all other predictor variables. Note that you **should not** include `rings` to predict `age`. *Explain why you shouldn't use `rings` to predict `age`.*

Steps for your recipe:

1.  dummy code any categorical predictors

2.  create interactions between

    -   `type` and `shucked_weight`,
    -   `longest_shell` and `diameter`,
    -   `shucked_weight` and `shell_weight`

3.  center all predictors, and

4.  scale all predictors.

You'll need to investigate the `tidymodels` documentation to find the appropriate step functions to use.

```{r}
abalone_train <- abalone_train %>% select(-rings)
abalone_recipe <- recipe(age ~ ., data = abalone_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  
  step_interact(terms = ~ type_M:shucked_weight) %>%
  step_interact(terms = ~ longest_shell:diameter) %>%
  step_interact(terms = ~ shucked_weight:shell_weight) %>%
  
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

```
You should not include rings in your dataset because rings and age are very directly related since age is just rings + 1.5. If we were to include rings, then this would lead to mutlicollinearity which would be be bad for our model. 

### Question 4

Create and store a linear regression object using the `"lm"` engine.

```{r}
lin_reg <- linear_reg() %>%
  set_engine("lm")
```

### Question 5

Create and store a KNN object using the `"kknn"` engine. Specify `k = 7`.

```{r}
knn <- nearest_neighbor(neighbors = 7) %>% 
  set_engine("kknn") %>%
  set_mode("regression")
```

### Question 6

Now, for each of these models (linear regression and KNN):

1.  set up an empty workflow,
2.  add the model, and
3.  add the recipe that you created in Question 3.

Note that you should be setting up two separate workflows.

Fit both models to the training set.

```{r}
linreg_wflow <- workflow() %>%
  add_model(lin_reg) %>%
  add_recipe(abalone_recipe)


linreg_fit <- fit(linreg_wflow, abalone_train)


knn_wflow <- workflow() %>% 
  add_model(knn) %>% 
  add_recipe(abalone_recipe)

knn_fit <- fit(knn_wflow, abalone_train)
```

### Question 7

Use your linear regression `fit()` object to predict the age of a hypothetical female abalone with longest_shell = 0.50, diameter = 0.10, height = 0.30, whole_weight = 4, shucked_weight = 1, viscera_weight = 2, and shell_weight = 1.

```{r}
linreg_pred <- predict(linreg_fit, new_data = data.frame(
  type = 'F',
  longest_shell = 0.50,
  diameter = 0.10,
  height = 0.30,
  whole_weight = 4,
  shucked_weight = 1,
  viscera_weight = 2,
  shell_weight = 1
))

linreg_pred
```
My linear regression model predicts that that new abalone will have an age of 21.9 years.

### Question 8

Now you want to assess your models' performance. To do this, use the `yardstick` package:

1.  Create a metric set that includes *R^2^*, RMSE (root mean squared error), and MAE (mean absolute error).
2.  Use `augment()` to create a tibble of your model's predicted values from the **testing data** along with the actual observed ages (these are needed to assess your model's performance).
3.  Finally, apply your metric set to the tibble, report the results, and interpret the *R\^2* value.

Repeat these steps once for the linear regression model and for the KNN model.

```{r}
library(yardstick)

metrics <- metric_set(rsq, rmse, mae)

linreg_predictions <- augment(linreg_fit, new_data = abalone_test)
linreg_metrics <- metrics(linreg_predictions, truth = age, estimate = .pred)
linreg_metrics

knn_predictions <- augment(knn_fit, new_data = abalone_test)
knn_metrics <- metrics(knn_predictions, truth = age, estimate = .pred)
knn_metrics
```
The value of R^2 for the linear regression model was 0.522 and the k nearest neighbors model was 0.462. In general, having an R^2 value closer to 1 means that a larger proportion of your variance is accounted for by the model, which is a good sign, and the opposite for values closer to 0. So in this case, having R^2 values that are closer to 0.5 isn't indicating that our models are doing very well because a lot of the variance isn't explained by the model, but they could be doing worse!

### Question 9

Which model performed better on the testing data? Explain why you think this might be. Are you surprised by any of your results? Why or why not?

The linear regression model performed better than the k nearest neighbors model overall. This means that there is a relatively linear relationship between the abalone age and the predictors that we were given in this dataset. Since linear regression seemed to be a better fit, then it makes sense why k nearest neighbors didn't do as well because might do better with a different dataset. I was a little bit surprised because I think of k nearest neighbors as a more complex predictor, but if the response and predictors have a linear relationship, then I guess it makes sense that the most accurate model would be linear regression and not knn. 



